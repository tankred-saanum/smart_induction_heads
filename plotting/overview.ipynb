{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6179a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dataclasses import dataclass\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from src.utils import first_order_markov_sequence, unique_second_order_markov_sequence, unique_third_order_markov_sequence, get_chunk_ids_in_order, get_chunks, get_chunks_3rd_order\n",
    "\n",
    "_ = torch.set_grad_enabled(False)\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
    "\n",
    "torch.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c974c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, attn_implementation=\"eager\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    chunk_size: int = 3\n",
    "    n_reps: int = 5\n",
    "    n_permute: int = 3\n",
    "    n_permute_primitive: int = 3\n",
    "\n",
    "basic_args = Args(n_reps=3)\n",
    "args = Args()\n",
    "third_order_args = Args(n_reps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.randint(model.config.vocab_size, size=(args.chunk_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = first_order_markov_sequence(tokens, basic_args)\n",
    "seq2, chunk2, perms2 = unique_second_order_markov_sequence(tokens, args, return_perms=True)\n",
    "seq3, chunk3, perms3, primitveperms3  = unique_third_order_markov_sequence(tokens, third_order_args, return_perms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e245fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_scores = torch.load(\"../data/learning_scores/markov3/Qwen2.5-1.5B/learning_scores.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e374b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1 # The rank of the layer/head pair to select (e.g., 10 for the 10th best)\n",
    "\n",
    "# Flatten the learning_scores tensor to find top k scores across all layers and heads\n",
    "flat_scores = learning_scores.flatten()\n",
    "\n",
    "# Get the top k scores and their flat indices\n",
    "# We need to find at least k top scores to select the k-th one.\n",
    "top_k_scores, top_k_flat_indices = torch.topk(flat_scores, k)\n",
    "\n",
    "# Convert the flat indices to 2D indices (layer, head)\n",
    "top_k_indices = [torch.unravel_index(i, learning_scores.shape) for i in top_k_flat_indices]\n",
    "\n",
    "# Select the k-th best layer/head pair (k-1 because of 0-indexing)\n",
    "chosen_layer, chosen_head = top_k_indices[k-1]\n",
    "chosen_score = top_k_scores[k-1]\n",
    "\n",
    "print(f\"Selected {k}-th top layer/head: ({chosen_layer.item()}, {chosen_head.item()}) with score {chosen_score.item():.4f}\")\n",
    "print(f\"\\nTop {k} layer/head pairs and their scores:\")\n",
    "for i in range(k):\n",
    "    layer, head = top_k_indices[i]\n",
    "    score = top_k_scores[i]\n",
    "    print(f\"  {i+1}. Layer {layer.item()}, Head {head.item()}: {score.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ad260",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_output = model(seq.unsqueeze(0).to(model.device),output_attentions=True).attentions\n",
    "seq_2_output = model(seq2.unsqueeze(0).to(model.device),output_attentions=True).attentions\n",
    "seq_2_chunks = get_chunks(seq_2_output[chosen_layer][0][chosen_head], args)\n",
    "seq_3_output = model(seq3.unsqueeze(0).to(model.device),output_attentions=True).attentions\n",
    "seq_3_chunks, seq_3_chunks_primitive = get_chunks_3rd_order(seq_3_output[chosen_layer][0][chosen_head], third_order_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_accs = torch.zeros(third_order_args.n_permute*third_order_args.n_reps)\n",
    "for i in range(1, seq_3_chunks.size(1)):\n",
    "    row_ideal = seq_3_chunks[i, :i]\n",
    "    row_model = seq_3_chunks[i, :i]\n",
    "    most_attn_idx = row_model.argmax(dim=0)\n",
    "    score = row_ideal[most_attn_idx]\n",
    "    #print(score.shape, 'score!')\n",
    "    \n",
    "    head_accs[i] = score\n",
    "    \n",
    "learning_score = head_accs[-10:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e972dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_attention_borders(ax, attention_matrix, token_sequence):\n",
    "    \"\"\"\n",
    "    Add colored borders to highlight a context-aware copying pattern.\n",
    "\n",
    "    For each token in the sequence (a query at position `i`), this function:\n",
    "    1. Identifies the actual next token in the sequence (at `i+1`), which is the prediction target.\n",
    "    2. Finds the position `max_col` where the query token `i` pays the most attention.\n",
    "    3. Checks if the token at `max_col` is the same as the target token from step 1.\n",
    "    4. If they match, the border is green (the head is attending to a previous\n",
    "       instance of the correct next token).\n",
    "    5. Otherwise, the border is red.\n",
    "    \"\"\"\n",
    "    seq_len = attention_matrix.shape[0]\n",
    "    # Loop up to the second-to-last token, as the last token has no \"next\" token.\n",
    "    for row in range(1, seq_len - 1):\n",
    "        # The token that should be predicted next.\n",
    "        target_token = token_sequence[row + 1]\n",
    "\n",
    "        # Find the column with maximum attention for the current row (query).\n",
    "        # We only look at positions up to the current one.\n",
    "        max_col = torch.argmax(attention_matrix[row, :row + 1]).item()\n",
    "\n",
    "        # The token that is actually being attended to.\n",
    "        attended_token = token_sequence[max_col]\n",
    "\n",
    "        # Check if the head is attending to a previous instance of the correct target token.\n",
    "        if attended_token == target_token:\n",
    "            border_color = 'green'\n",
    "        else:\n",
    "            border_color = 'red'\n",
    "\n",
    "        # Add a rectangle border around the cell with the highest attention.\n",
    "        rect = plt.Rectangle((max_col, row), 1, 1,\n",
    "                           fill=True, edgecolor=border_color, facecolor=border_color,\n",
    "                           linewidth=1, zorder=10, clip_on=False, alpha=0.5)\n",
    "        ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604366e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_copying_attention_borders(ax, attention_matrix, token_sequence):\n",
    "    \"\"\"\n",
    "    Add colored borders to cells with maximum attention per row,\n",
    "    highlighting patterns of attending to previous instances of the same token.\n",
    "\n",
    "    For each token in the sequence (a query at position `row`), this function:\n",
    "    1. Finds the position of the token with the highest attention score (`max_col`).\n",
    "    2. Checks if the token at `max_col` is the same as the current token.\n",
    "    3. If it is, the border is green (indicating attention to a previous instance of itself).\n",
    "    4. Otherwise, the border is red.\n",
    "    \"\"\"\n",
    "    seq_len = attention_matrix.shape[0]\n",
    "    for row in range(1, seq_len):  # Start from 1, as token 0 has no history\n",
    "        current_token = token_sequence[row]\n",
    "        \n",
    "        # Find the column with maximum attention for this row, up to the current position\n",
    "        max_col = torch.argmax(attention_matrix[row, :row]).item()\n",
    "\n",
    "        # Get the token ID at the position of maximum attention\n",
    "        attended_token = token_sequence[max_col]\n",
    "\n",
    "        # Check if the head is attending to a previous instance of the same token\n",
    "        if attended_token == current_token:\n",
    "            border_color = 'green'\n",
    "        else:\n",
    "            border_color = 'red'\n",
    "\n",
    "        # Add a rectangle border around the cell with the highest attention\n",
    "        # specify both edgecolor and the filling color\n",
    "        rect = plt.Rectangle((max_col, row), 1, 1,\n",
    "                           fill=True, edgecolor=border_color, facecolor=border_color,\n",
    "                           linewidth=1, zorder=10, clip_on=False, alpha=0.5)\n",
    "        ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754adc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(30, 30))\n",
    "for ax in axs.flatten():\n",
    "    ax.tick_params(axis=u'both', which=u'both',length=0)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i < j:\n",
    "            axs[i, j].set_xticks([])\n",
    "            axs[i, j].set_yticks([])\n",
    "            axs[i, j].set_xlabel(\"\")\n",
    "            axs[i, j].set_ylabel(\"\")\n",
    "            for spine in axs[i, j].spines.values():\n",
    "                spine.set_visible(False)\n",
    "\n",
    "unique_tokens = list(set(seq.tolist()))\n",
    "unique_tokens.sort()\n",
    "token_to_letter = {token: chr(97+i) for i, token in enumerate(unique_tokens)}\n",
    "seq_letters = [token_to_letter[token] for token in seq.tolist()]\n",
    "sns.heatmap(seq_output[chosen_layer][0][chosen_head].cpu().float(),cbar=False, cmap=\"copper\",linewidths=1, linecolor='black', ax=axs[0,0])\n",
    "add_attention_borders(axs[0,0], seq_output[chosen_layer][0][chosen_head].cpu().float(), seq)\n",
    "axs[0,0].set_xticks(torch.arange(len(seq_letters))+0.5)\n",
    "axs[0,0].set_yticks(torch.arange(len(seq_letters))+0.5)\n",
    "axs[0,0].set_xticklabels([f\"${letter}$\" for letter in seq_letters], fontsize=30)\n",
    "axs[0,0].set_yticklabels([f\"${letter}$\" for letter in seq_letters], fontsize=30, rotation=0)\n",
    "\n",
    "\n",
    "sns.heatmap(seq_2_chunks.cpu().float(), cbar=False, cmap=\"copper\", linewidths=1, linecolor='black', ax=axs[1,0])\n",
    "chunk_ids = get_chunk_ids_in_order(perms2)\n",
    "add_copying_attention_borders(axs[1,0], seq_2_chunks.cpu().float(), torch.tensor(chunk_ids))\n",
    "id_to_letter = {i: chr(945 + i) for i in range(len(set(chunk_ids)))}\n",
    "axs[1,0].set_xticks(torch.arange(len(chunk_ids)) + 0.5)\n",
    "axs[1,0].set_yticks(torch.arange(len(chunk_ids)) + 0.5)\n",
    "axs[1,0].set_xticklabels([f\"${id_to_letter[id]}$\" for id in chunk_ids], fontsize=30)\n",
    "axs[1,0].set_yticklabels([f\"${id_to_letter[id]}$\" for id in chunk_ids], fontsize=30, rotation=0)\n",
    "\n",
    "unique_tokens = list(set(seq2.tolist()))\n",
    "unique_tokens.sort()\n",
    "token_to_letter = {token: chr(97 + i) for i, token in enumerate(unique_tokens)}\n",
    "seq2_letters = [token_to_letter[token] for token in seq2.tolist()]\n",
    "\n",
    "sns.heatmap(seq_2_output[chosen_layer][0][chosen_head].cpu().float(), cbar=False, cmap=\"copper\", linewidths=1, linecolor='black', ax=axs[1,1])\n",
    "add_attention_borders(axs[1,1], seq_2_output[chosen_layer][0][chosen_head].cpu().float(), seq2)\n",
    "axs[1,1].set_xticks(torch.arange(len(seq2_letters)) + 0.5)\n",
    "axs[1,1].set_yticks(torch.arange(len(seq2_letters)) + 0.5)\n",
    "axs[1,1].set_xticklabels([f\"${letter}$\" for letter in seq2_letters], fontsize=15, rotation=0)\n",
    "axs[1,1].set_yticklabels([f\"${letter}$\" for letter in seq2_letters], fontsize=15, rotation=0)\n",
    "\n",
    "# every args.chunk_size, draw a white horizontal and vertical line to separate chunks\n",
    "for i in range(1, args.n_reps * args.n_permute):\n",
    "    axs[1,1].axhline(i * args.chunk_size, color='white', linewidth=2, zorder=10)\n",
    "    axs[1,1].axvline(i * args.chunk_size, color='white', linewidth=2, zorder=10)\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(seq_3_chunks.cpu().float(), cbar=False, cmap=\"copper\", linewidths=1, linecolor='black', ax=axs[2,0])\n",
    "chunk_ids = get_chunk_ids_in_order(perms3)\n",
    "# use uppercase phi, psi, and omega for the top level chunk ids\n",
    "# make sure we use the uppercase letters in latex format\n",
    "\n",
    "greek_letters = ['\\u03A9', '\\u03C8', '\\u03C6'] # Ω, ψ, φ\n",
    "id_to_letter = {i: greek_letters[i] for i in range(len(set(chunk_ids)))}\n",
    "add_copying_attention_borders(axs[2,0], seq_3_chunks.cpu().float(), torch.tensor(chunk_ids))\n",
    "axs[2,0].set_xticks(torch.arange(len(chunk_ids)) + 0.5)\n",
    "axs[2,0].set_yticks(torch.arange(len(chunk_ids)) + 0.5)\n",
    "axs[2,0].set_xticklabels([f\"${id_to_letter[id]}$\" for id in chunk_ids], fontsize=30)\n",
    "axs[2,0].set_yticklabels([f\"${id_to_letter[id]}$\" for id in chunk_ids], fontsize=30, rotation=0)\n",
    "# Second subplot for the chunked attention (averaged)\n",
    "sns.heatmap(seq_3_chunks_primitive.cpu().float(), cbar=False, cmap=\"copper\", linewidths=1, linecolor='black', ax=axs[2,1])\n",
    "chunk_ids = get_chunk_ids_in_order(primitveperms3)\n",
    "add_copying_attention_borders(axs[2,1], seq_3_chunks_primitive.cpu().float(), torch.tensor(chunk_ids))\n",
    "id_to_letter = {i: chr(945 + i) for i in range(len(set(chunk_ids)))}\n",
    "axs[2,1].set_xticks(torch.arange(len(chunk_ids)) + 0.5)\n",
    "axs[2,1].set_yticks(torch.arange(len(chunk_ids)) + 0.5)\n",
    "axs[2,1].set_xticklabels([f\"${id_to_letter[id]}$\" for id in chunk_ids], fontsize=15, rotation=0)\n",
    "axs[2,1].set_yticklabels([f\"${id_to_letter[id]}$\" for id in chunk_ids], fontsize=15, rotation=0)\n",
    "# Third subplot for the full attention matrix\n",
    "for i in range(1, third_order_args.n_reps * third_order_args.n_permute):\n",
    "    axs[2,1].axhline(i * third_order_args.chunk_size, color='white', linewidth=2, zorder=10)\n",
    "    axs[2,1].axvline(i * third_order_args.chunk_size, color='white', linewidth=2, zorder=10)\n",
    "unique_tokens = list(set(seq3.tolist()))\n",
    "unique_tokens.sort()\n",
    "token_to_letter = {token: chr(97+i) for i, token in enumerate(unique_tokens)}\n",
    "seq3_letters = [token_to_letter[token] for token in seq3.tolist()] \n",
    "sns.heatmap(seq_3_output[chosen_layer][0][chosen_head].cpu().float(), cbar=False, cmap=\"copper\", linewidths=1, linecolor='black', ax=axs[2,2])\n",
    "add_attention_borders(axs[2,2], seq_3_output[chosen_layer][0][chosen_head].cpu().float(), seq3)\n",
    "\n",
    "axs[2,2].set_xticks(torch.arange(len(seq3_letters)) + 0.5)\n",
    "axs[2,2].set_yticks(torch.arange(len(seq3_letters)) + 0.5)\n",
    "axs[2,2].set_xticklabels([f\"${letter}$\" for letter in seq3_letters], fontsize=8, rotation=0)\n",
    "axs[2,2].set_yticklabels([f\"${letter}$\" for letter in seq3_letters], fontsize=8, rotation=0)\n",
    "# Third subplot for the full attention matrix\n",
    "for i in range(1, third_order_args.n_reps * third_order_args.n_permute * third_order_args.n_permute_primitive):\n",
    "    axs[2,2].axhline(i * third_order_args.chunk_size * third_order_args.n_permute, color='white', linewidth=2, zorder=10)\n",
    "    axs[2,2].axvline(i * third_order_args.chunk_size* third_order_args.n_permute , color='white', linewidth=2, zorder=10)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"../figures/overview.svg\", format=\"svg\")\n",
    "# plt.savefig(\"../figures/overview.png\", format=\"png\")\n",
    "# plt.savefig(\"../figures/overview.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(30, 30))\n",
    "for ax in axs.flatten():\n",
    "    ax.tick_params(axis=u'both', which=u'both',length=0)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i < j:\n",
    "            axs[i, j].set_xticks([])\n",
    "            axs[i, j].set_yticks([])\n",
    "            axs[i, j].set_xlabel(\"\")\n",
    "            axs[i, j].set_ylabel(\"\")\n",
    "            for spine in axs[i, j].spines.values():\n",
    "                spine.set_visible(False)\n",
    "\n",
    "unique_tokens = list(set(seq.tolist()))\n",
    "unique_tokens.sort()\n",
    "token_to_letter = {token: chr(97+i) for i, token in enumerate(unique_tokens)}\n",
    "seq_letters = [token_to_letter[token] for token in seq.tolist()]\n",
    "sns.heatmap(seq_output[chosen_layer][0][chosen_head].cpu().float(),cbar=False, cmap=\"copper\",linewidths=1, linecolor='black', ax=axs[0,0])\n",
    "add_attention_borders(axs[0,0], seq_output[chosen_layer][0][chosen_head].cpu().float(), seq)\n",
    "axs[0,0].set_xticks(torch.arange(len(seq_letters))+0.5)\n",
    "axs[0,0].set_yticks(torch.arange(len(seq_letters))+0.5)\n",
    "axs[0,0].set_xticklabels([f\"${letter}$\" for letter in seq_letters], fontsize=30)\n",
    "axs[0,0].set_yticklabels([f\"${letter}$\" for letter in seq_letters], fontsize=30, rotation=0)\n",
    "\n",
    "\n",
    "sns.heatmap(seq_2_chunks.cpu().float(), cbar=False, cmap=\"copper\", linewidths=1, linecolor='black', ax=axs[1,0])\n",
    "chunk_ids = get_chunk_ids_in_order(perms2)\n",
    "add_copying_attention_borders(axs[1,0], seq_2_chunks.cpu().float(), torch.tensor(chunk_ids))\n",
    "id_to_letter = {i: chr(945 + i) for i in range(len(set(chunk_ids)))}\n",
    "axs[1,0].set_xticks(torch.arange(len(chunk_ids)) + 0.5)\n",
    "axs[1,0].set_yticks(torch.arange(len(chunk_ids)) + 0.5)\n",
    "axs[1,0].set_xticklabels([f\"${id_to_letter[id]}$\" for id in chunk_ids], fontsize=30)\n",
    "axs[1,0].set_yticklabels([f\"${id_to_letter[id]}$\" for id in chunk_ids], fontsize=30, rotation=0)\n",
    "\n",
    "unique_tokens = list(set(seq2.tolist()))\n",
    "unique_tokens.sort()\n",
    "token_to_letter = {token: chr(97 + i) for i, token in enumerate(unique_tokens)}\n",
    "seq2_letters = [token_to_letter[token] for token in seq2.tolist()]\n",
    "\n",
    "sns.heatmap(seq_2_output[chosen_layer][0][chosen_head].cpu().float(), cbar=False, cmap=\"copper\", linewidths=1, linecolor='black', ax=axs[1,1])\n",
    "add_attention_borders(axs[1,1], seq_2_output[chosen_layer][0][chosen_head].cpu().float(), seq2)\n",
    "axs[1,1].set_xticks(torch.arange(len(seq2_letters)) + 0.5)\n",
    "axs[1,1].set_yticks(torch.arange(len(seq2_letters)) + 0.5)\n",
    "axs[1,1].set_xticklabels([f\"${letter}$\" for letter in seq2_letters], fontsize=15, rotation=0)\n",
    "axs[1,1].set_yticklabels([f\"${letter}$\" for letter in seq2_letters], fontsize=15, rotation=0)\n",
    "\n",
    "# every args.chunk_size, draw a white horizontal and vertical line to separate chunks\n",
    "for i in range(1, args.n_reps * args.n_permute):\n",
    "    axs[1,1].axhline(i * args.chunk_size, color='white', linewidth=2, zorder=10)\n",
    "    axs[1,1].axvline(i * args.chunk_size, color='white', linewidth=2, zorder=10)\n",
    "\n",
    "# Slice the third order matrices to show only the lower right quarter\n",
    "seq_len_3 = seq_3_chunks.shape[0]\n",
    "quarter_start_3 = int(seq_len_3 // 1.1)\n",
    "seq_3_chunks_quarter = seq_3_chunks[quarter_start_3:, quarter_start_3:]\n",
    "\n",
    "seq_len_3_prim = seq_3_chunks_primitive.shape[0]\n",
    "quarter_start_3_prim = int(seq_len_3_prim // 1.1)\n",
    "seq_3_chunks_primitive_quarter = seq_3_chunks_primitive[quarter_start_3_prim:, quarter_start_3_prim:]\n",
    "\n",
    "seq_len_3_full = seq_3_output[chosen_layer][0][chosen_head].shape[0]\n",
    "quarter_start_3_full = int(seq_len_3_full // 1.1)\n",
    "seq_3_output_quarter = seq_3_output[chosen_layer][0][chosen_head][quarter_start_3_full:, quarter_start_3_full:]\n",
    "\n",
    "# Also slice the corresponding labels and sequences\n",
    "chunk_ids_3 = get_chunk_ids_in_order(perms3)\n",
    "chunk_ids_3_quarter = chunk_ids_3[quarter_start_3:]\n",
    "\n",
    "chunk_ids_3_prim = get_chunk_ids_in_order(primitveperms3)\n",
    "chunk_ids_3_prim_quarter = chunk_ids_3_prim[quarter_start_3_prim:]\n",
    "\n",
    "unique_tokens = list(set(seq3.tolist()))\n",
    "unique_tokens.sort()\n",
    "token_to_letter = {token: chr(97+i) for i, token in enumerate(unique_tokens)}\n",
    "seq3_letters = [token_to_letter[token] for token in seq3.tolist()]\n",
    "seq3_letters_quarter = seq3_letters[quarter_start_3_full:]\n",
    "seq3_quarter = seq3[quarter_start_3_full:]\n",
    "\n",
    "# Plot the quarters for the last row\n",
    "sns.heatmap(seq_3_chunks_quarter.cpu().float(), cbar=False, cmap=\"copper\", linewidths=1, linecolor='black', ax=axs[2,0])\n",
    "# use uppercase phi, psi, and omega for the top level chunk ids\n",
    "greek_letters = ['\\u03A9', '\\u03C8', '\\u03C6'] # Ω, ψ, φ\n",
    "id_to_letter = {i: greek_letters[i] for i in range(len(set(chunk_ids_3)))}\n",
    "add_copying_attention_borders(axs[2,0], seq_3_chunks_quarter.cpu().float(), torch.tensor(chunk_ids_3_quarter))\n",
    "axs[2,0].set_xticks(torch.arange(len(chunk_ids_3_quarter)) + 0.5)\n",
    "axs[2,0].set_yticks(torch.arange(len(chunk_ids_3_quarter)) + 0.5)\n",
    "axs[2,0].set_xticklabels([f\"${id_to_letter[id]}$\" for id in chunk_ids_3_quarter], fontsize=30)\n",
    "axs[2,0].set_yticklabels([f\"${id_to_letter[id]}$\" for id in chunk_ids_3_quarter], fontsize=30, rotation=0)\n",
    "\n",
    "# Second subplot for the chunked attention (averaged) - quarter\n",
    "sns.heatmap(seq_3_chunks_primitive_quarter.cpu().float(), cbar=False, cmap=\"copper\", linewidths=1, linecolor='black', ax=axs[2,1])\n",
    "add_copying_attention_borders(axs[2,1], seq_3_chunks_primitive_quarter.cpu().float(), torch.tensor(chunk_ids_3_prim_quarter))\n",
    "id_to_letter = {i: chr(945 + i) for i in range(len(set(chunk_ids_3_prim)))}\n",
    "axs[2,1].set_xticks(torch.arange(len(chunk_ids_3_prim_quarter)) + 0.5)\n",
    "axs[2,1].set_yticks(torch.arange(len(chunk_ids_3_prim_quarter)) + 0.5)\n",
    "axs[2,1].set_xticklabels([f\"${id_to_letter[id]}$\" for id in chunk_ids_3_prim_quarter], fontsize=15, rotation=0)\n",
    "axs[2,1].set_yticklabels([f\"${id_to_letter[id]}$\" for id in chunk_ids_3_prim_quarter], fontsize=15, rotation=0)\n",
    "\n",
    "# Third subplot for the full attention matrix - quarter\n",
    "sns.heatmap(seq_3_output_quarter.cpu().float(), cbar=False, cmap=\"copper\", linewidths=1, linecolor='black', ax=axs[2,2])\n",
    "add_attention_borders(axs[2,2], seq_3_output_quarter.cpu().float(), seq3_quarter)\n",
    "axs[2,2].set_xticks(torch.arange(len(seq3_letters_quarter)) + 0.5)\n",
    "axs[2,2].set_yticks(torch.arange(len(seq3_letters_quarter)) + 0.5)\n",
    "axs[2,2].set_xticklabels([f\"${letter}$\" for letter in seq3_letters_quarter], fontsize=8, rotation=0)\n",
    "axs[2,2].set_yticklabels([f\"${letter}$\" for letter in seq3_letters_quarter], fontsize=8, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"../figures/overview.svg\", format=\"svg\")\n",
    "# plt.savefig(\"../figures/overview.png\", format=\"png\")\n",
    "# plt.savefig(\"../figures/overview.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffc837",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_3 // 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6edec5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
